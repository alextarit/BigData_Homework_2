{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f9ece4-83c7-486a-84b6-192fca40d533",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "## Входные данные \n",
    "- Файл с данными по оттоку телеком оператора в США (churn.csv)\n",
    "- Справочник с названиями штатов (state.json)\n",
    "- Справочник с численностью населения территорий (определяется полем area code) внутри штатов (state.json)\n",
    "- Террия с численностью населения меньше 10_000 считается **мелкой**\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Посчитать количество отточных и неотточных абонентов (поле churn), исключив **мелкие** территории\n",
    "2. Отчет должен быть выполнен в разрезе **каждого штата** с его полным наименованием\n",
    "3. Описать возникающие узкие места при выполнении данной операции\n",
    "4. Применить один из способов оптимизации для ускорения выполнения запроса (при допущении, что справочник численности населения **сильно меньше** основных данных)\n",
    "5. Если существует еще какой-то способ, применить также и его отдельно от п.4 (при допущении, что справочник численности населения **сопоставим по размеру** с основными данными)\n",
    "6. Кратко описать реализованные способы и в чем их практическая польза\n",
    "\n",
    "P.S. Одним из выбранных способов должен быть Bucket specific join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd39bc-4dbf-4deb-bab3-2a3ef7ee67ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b06d19-8478-4df4-bb2d-99dfa4f07aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1909f1-a5fa-4393-8213-a0c8bca6525d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1e405e5-032d-4d51-bee7-62d9653bc748;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.12.365 in central\n",
      ":: resolution report :: resolve 117ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.12.365 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c1e405e5-032d-4d51-bee7-62d9653bc748\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "24/06/20 09:37:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"sykorole_test\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .config(\"spark.executor.memory\", \"450M\") \\\n",
    "    .config(\"spark.driver.memory\", \"450M\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", True) \\\n",
    "    .config('spark.jars.packages', [\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2\",\n",
    "        \"com.amazonaws:aws-java-sdk-pom:1.12.365\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\",\n",
    "    ]\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"XosDG9F6D3nDSHjOWd7H\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"I2cpVw7DWhc6SpswCWcoB554vlYqnbq4iYSKYhPk\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67bdeb-fb83-4354-b8d3-97b02e90e9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b58da6b-e2ff-4cd6-9995-c4d35e5c2745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 09:37:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/churn.csv\")\n",
    "state_dict = spark.read.json(\"s3a://input/data/state.json\").withColumnRenamed(\"state_id\", \"state\")\n",
    "pop_dict = spark.read.json(\"s3a://input/data/population.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "88405ccd-aa66-455b-a526-4110609a238e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|state_id|state_name|\n",
      "+--------+----------+\n",
      "|      AL|   Alabama|\n",
      "+--------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_dict.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1893fd8a-d174-46b4-b075-5c4822e746f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|area code|population|\n",
      "+---------+----------+\n",
      "|      131|     15742|\n",
      "+---------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_dict.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c318a74f-bd6a-48d6-85ae-cf32a16199b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_THRESHOLD = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d80488-325b-4dd5-8990-24bec5fce72c",
   "metadata": {},
   "source": [
    "#### Простое решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02cb43c-9162-48fb-a97a-e0e03ff39d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_population = churn_df.join(pop_dict, on=\"area code\", how=\"left\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc79dff-771a-40db-80a1-bf55151657a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_population.filter(F.col(\"population\").isNull()).count()  # убедились, что нет пропущенных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a109e-37ac-4a3a-8262-19489d3c4cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f35bf79-939e-4497-8000-06ac0f40ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_with_population \\\n",
    "    .join(state_dict, on=\"state\", how=\"left\") \\\n",
    "    .groupBy(\"state_name\", \"churn\") \\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7db3a2b2-6042-4cb2-a59e-6afa3e73ebaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(9) Sort [state_name#72 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(state_name#72 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1667]\n",
      "   +- *(8) HashAggregate(keys=[state_name#72, churn#945], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(state_name#72, churn#945, 200), ENSURE_REQUIREMENTS, [plan_id=1663]\n",
      "         +- *(7) HashAggregate(keys=[state_name#72, churn#945], functions=[partial_count(1)])\n",
      "            +- *(7) Project [churn#945, state_name#72]\n",
      "               +- *(7) SortMergeJoin [state#925], [state#75], LeftOuter\n",
      "                  :- *(4) Sort [state#925 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(state#925, 200), ENSURE_REQUIREMENTS, [plan_id=1645]\n",
      "                  :     +- *(3) Project [state#925, churn#945]\n",
      "                  :        +- *(3) SortMergeJoin [area code#927], [area code#970], Inner\n",
      "                  :           :- *(1) Sort [area code#927 ASC NULLS FIRST], false, 0\n",
      "                  :           :  +- *(1) Filter isnotnull(area code#927)\n",
      "                  :           :     +- *(1) ColumnarToRow\n",
      "                  :           :        +- FileScan parquet spark_catalog.default.churn_bucketed[state#925,area code#927,churn#945] Batched: true, Bucketed: true, DataFilters: [isnotnull(area code#927)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://input/data/bucketed/churn], PartitionFilters: [], PushedFilters: [IsNotNull(`area code`)], ReadSchema: struct<state:string,area code:string,churn:string>, SelectedBucketsCount: 50 out of 50\n",
      "                  :           +- *(2) Sort [area code#970 ASC NULLS FIRST], false, 0\n",
      "                  :              +- *(2) Project [area code#970]\n",
      "                  :                 +- *(2) Filter ((isnotnull(population#971L) AND (population#971L > 10000)) AND isnotnull(area code#970))\n",
      "                  :                    +- *(2) ColumnarToRow\n",
      "                  :                       +- FileScan parquet spark_catalog.default.pop[area code#970,population#971L] Batched: true, Bucketed: true, DataFilters: [isnotnull(population#971L), (population#971L > 10000), isnotnull(area code#970)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://input/data/bucketed/population], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,10000), IsNotNull(`area code`)], ReadSchema: struct<area code:string,population:bigint>, SelectedBucketsCount: 50 out of 50\n",
      "                  +- *(6) Sort [state#75 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(state#75, 200), ENSURE_REQUIREMENTS, [plan_id=1654]\n",
      "                        +- *(5) Project [state_id#71 AS state#75, state_name#72]\n",
      "                           +- *(5) Filter isnotnull(state_id#71)\n",
      "                              +- FileScan json [state_id#71,state_name#72] Batched: false, DataFilters: [isnotnull(state_id#71)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/state.json], PartitionFilters: [], PushedFilters: [IsNotNull(state_id)], ReadSchema: struct<state_id:string,state_name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf062c-23e6-42ac-925f-acee6e124792",
   "metadata": {},
   "source": [
    "- в плане видим две операции перемешивания данных (exchange)\n",
    "- применение sort merge join\n",
    "- перекос в spark ui в разделе stages -> event timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab8753d9-a1e9-4a6b-913f-360e41099e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:======================================>               (143 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+\n",
      "|state_name|churn|cnt|\n",
      "+----------+-----+---+\n",
      "|Alabama   |False|58 |\n",
      "|Alabama   |True |7  |\n",
      "|Alaska    |True |3  |\n",
      "|Alaska    |False|44 |\n",
      "+----------+-----+---+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 12:29:18 ERROR TaskSchedulerImpl: Lost executor 0 on 172.20.0.2: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/06/20 12:35:16 ERROR TaskSchedulerImpl: Lost executor 1 on 172.20.0.2: worker lost: Not receiving heartbeat for 60 seconds\n",
      "24/06/20 12:51:52 ERROR TaskSchedulerImpl: Lost executor 2 on 172.20.0.2: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "result.show(4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5213e1db-bd22-4990-96f3-1e20e8f66e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ddb04-8a00-4aab-81e2-100ce5e16bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5df2e28-5611-4e28-a569-1c81dbdbf274",
   "metadata": {},
   "source": [
    "#### Оптимизация - broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb819a1-1ead-4752-87b6-12334e6c2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оба справочника мелкие, поэтому можно broadcast... но минимум для pop_dict - в нем перекошенный ключ area_code\n",
    "pop_dict = F.broadcast(pop_dict)\n",
    "state_dict = F.broadcast(state_dict)  # state равномерно распределен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fafd5ca2-3938-4fb7-ac72-2cffa0fa5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_population = churn_df.join(pop_dict, on=\"area code\", how=\"left\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1d3eb6a-c42c-4b70-b7ea-5d0e69524d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_result = data_with_population \\\n",
    "    .join(state_dict, on=\"state\", how=\"left\") \\\n",
    "    .groupBy(\"churn\") \\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d95e08f-ab9c-408b-a2b2-fe61a9c68390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[churn#401], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(churn#401, 200), ENSURE_REQUIREMENTS, [plan_id=868]\n",
      "   +- *(3) HashAggregate(keys=[churn#401], functions=[partial_count(1)])\n",
      "      +- *(3) Project [churn#401]\n",
      "         +- *(3) BroadcastHashJoin [state#381], [state#438], LeftOuter, BuildRight, false\n",
      "            :- *(3) Project [state#381, churn#401]\n",
      "            :  +- *(3) BroadcastHashJoin [cast(area code#383 as bigint)], [area code#450L], Inner, BuildRight, false\n",
      "            :     :- *(3) Filter isnotnull(area code#383)\n",
      "            :     :  +- FileScan csv [state#381,area code#383,churn#401] Batched: false, DataFilters: [isnotnull(area code#383)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3a://input/data/churn.csv], PartitionFilters: [], PushedFilters: [IsNotNull(area code)], ReadSchema: struct<state:string,area code:string,churn:string>\n",
      "            :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=854]\n",
      "            :        +- *(1) Project [area code#450L]\n",
      "            :           +- *(1) Filter ((isnotnull(population#451L) AND (population#451L > 10000)) AND isnotnull(area code#450L))\n",
      "            :              +- FileScan json [area code#450L,population#451L] Batched: false, DataFilters: [isnotnull(population#451L), (population#451L > 10000), isnotnull(area code#450L)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/population.json], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,10000), IsNotNull(area code)], ReadSchema: struct<area code:bigint,population:bigint>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=862]\n",
      "               +- *(2) Project [state_id#434 AS state#438]\n",
      "                  +- *(2) Filter isnotnull(state_id#434)\n",
      "                     +- FileScan json [state_id#434] Batched: false, DataFilters: [isnotnull(state_id#434)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/state.json], PartitionFilters: [], PushedFilters: [IsNotNull(state_id)], ReadSchema: struct<state_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca08fa-0dea-4149-b9dc-0b6f81038015",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_result.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0682d8-88a7-4747-bfde-0898a2e7ce27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3ae44ae-23d7-4b59-89a6-e11e31e3f5fc",
   "metadata": {},
   "source": [
    "#### Bucket specific join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2304cd89-b0a3-4e9f-a280-2be6ac1e471f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(50, \"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/bucketed/churn\") \\\n",
    "    .saveAsTable(\"churn_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd1686-36b9-4f41-b08a-ec0c52a514f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db956be4-e50c-40fe-8f65-752906d0eef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pop_dict \\\n",
    "    .withColumn(\"area code\", F.col(\"area code\").cast(\"string\")) \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(50, \"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/bucketed/population\") \\\n",
    "    .saveAsTable(\"pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e6fb6-c14e-4633-8a51-97226cd928b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40718745-96a0-43f7-bcc5-782e76e2bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# churn_bucketed_df = spark.read.parquet(\"s3a://input/data/bucketed/churn\")\n",
    "# pop_bucketed_dict = spark.read.parquet(\"s3a://input/data/bucketed/population\")\n",
    "\n",
    "churn_bucketed_df = spark.table(\"churn_bucketed\")\n",
    "pop_bucketed_dict = spark.table(\"pop\")  # бакетирование работает только на таблицах, не на файлах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b47d06-9884-4f5a-8404-8a6b32ad3378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68c1cff3-7753-41e0-bf3d-d76d4902c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_population = churn_bucketed_df.join(pop_bucketed_dict, on=\"area code\", how=\"left\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "244b825a-299d-4786-b941-623d7ac013cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketed_result = data_with_population \\\n",
    "    .join(state_dict, on=\"state\", how=\"left\") \\\n",
    "    .groupBy(\"churn\") \\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0a3265f-e7aa-4ddc-af07-1711fcf7eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(8) HashAggregate(keys=[churn#945], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(churn#945, 200), ENSURE_REQUIREMENTS, [plan_id=1290]\n",
      "   +- *(7) HashAggregate(keys=[churn#945], functions=[partial_count(1)])\n",
      "      +- *(7) Project [churn#945]\n",
      "         +- *(7) SortMergeJoin [state#925], [state#75], LeftOuter\n",
      "            :- *(4) Sort [state#925 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(state#925, 200), ENSURE_REQUIREMENTS, [plan_id=1272]\n",
      "            :     +- *(3) Project [state#925, churn#945]\n",
      "            :        +- *(3) SortMergeJoin [area code#927], [area code#970], Inner\n",
      "            :           :- *(1) Sort [area code#927 ASC NULLS FIRST], false, 0\n",
      "            :           :  +- *(1) Filter isnotnull(area code#927)\n",
      "            :           :     +- *(1) ColumnarToRow\n",
      "            :           :        +- FileScan parquet spark_catalog.default.churn_bucketed[state#925,area code#927,churn#945] Batched: true, Bucketed: true, DataFilters: [isnotnull(area code#927)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://input/data/bucketed/churn], PartitionFilters: [], PushedFilters: [IsNotNull(`area code`)], ReadSchema: struct<state:string,area code:string,churn:string>, SelectedBucketsCount: 50 out of 50\n",
      "            :           +- *(2) Sort [area code#970 ASC NULLS FIRST], false, 0\n",
      "            :              +- *(2) Project [area code#970]\n",
      "            :                 +- *(2) Filter ((isnotnull(population#971L) AND (population#971L > 10000)) AND isnotnull(area code#970))\n",
      "            :                    +- *(2) ColumnarToRow\n",
      "            :                       +- FileScan parquet spark_catalog.default.pop[area code#970,population#971L] Batched: true, Bucketed: true, DataFilters: [isnotnull(population#971L), (population#971L > 10000), isnotnull(area code#970)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://input/data/bucketed/population], PartitionFilters: [], PushedFilters: [IsNotNull(population), GreaterThan(population,10000), IsNotNull(`area code`)], ReadSchema: struct<area code:string,population:bigint>, SelectedBucketsCount: 50 out of 50\n",
      "            +- *(6) Sort [state#75 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(state#75, 200), ENSURE_REQUIREMENTS, [plan_id=1281]\n",
      "                  +- *(5) Project [state_id#71 AS state#75]\n",
      "                     +- *(5) Filter isnotnull(state_id#71)\n",
      "                        +- FileScan json [state_id#71] Batched: false, DataFilters: [isnotnull(state_id#71)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3a://input/data/state.json], PartitionFilters: [], PushedFilters: [IsNotNull(state_id)], ReadSchema: struct<state_id:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обязательно перезапустить сессию, чтобы broadcast не применился\n",
    "bucketed_result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffbcb3-842e-44a7-8fac-d3c0550cce56",
   "metadata": {},
   "source": [
    "- видим, что нет шага exchange по area code -> исключили шаффл + избежали джоина по перекошенному ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d1535-6946-43b9-9eeb-29e1f43ac8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
