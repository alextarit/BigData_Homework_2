{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41248ca-5911-4ecd-83b0-9565b724df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9eb73c-7ffb-4cfc-8071-a3d77c3ab9bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-909a7119-9757-45f3-ad92-496a7d623345;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.12.365 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 422ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.12.365 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-909a7119-9757-45f3-ad92-496a7d623345\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/7ms)\n",
      "24/06/20 17:35:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"sykorole_test\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .config(\"spark.executor.memory\", \"450M\") \\\n",
    "    .config(\"spark.driver.memory\", \"450M\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", True) \\\n",
    "    .config('spark.jars.packages',\n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2,\" +\n",
    "        \"com.amazonaws:aws-java-sdk-pom:1.12.365,\" +\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"XosDG9F6D3nDSHjOWd7H\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"I2cpVw7DWhc6SpswCWcoB554vlYqnbq4iYSKYhPk\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70842559-ffb2-4e12-93c6-da41c6e4afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://14900702aaa8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sykorole_test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffffa83003d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4aaf36-bdd4-4119-a6c3-570cb219ea60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93b8aa75-263a-4946-9d15-ab6aeff2cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"s3a://lesson6/fda_approved_food_items_w_nutrient_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e01d1ac-749f-426b-9bb8-c2ef7a1ceaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1841897"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8564beab-4140-4350-8c4d-7a6dbe08d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fdc_id: string (nullable = true)\n",
      " |-- brand_owner: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- ingredients: string (nullable = true)\n",
      " |-- gtin_upc: string (nullable = true)\n",
      " |-- serving_size: string (nullable = true)\n",
      " |-- serving_size_unit: string (nullable = true)\n",
      " |-- branded_food_category: string (nullable = true)\n",
      " |-- modified_date: string (nullable = true)\n",
      " |-- available_date: string (nullable = true)\n",
      " |-- Calcium, Ca-MG: string (nullable = true)\n",
      " |-- Carbohydrate, by difference-G: string (nullable = true)\n",
      " |-- Cholesterol-MG: string (nullable = true)\n",
      " |-- Copper, Cu-MG: string (nullable = true)\n",
      " |-- Energy-KCAL: string (nullable = true)\n",
      " |-- Fatty acids, total monounsaturated-G: string (nullable = true)\n",
      " |-- Fatty acids, total polyunsaturated-G: string (nullable = true)\n",
      " |-- Fatty acids, total saturated-G: string (nullable = true)\n",
      " |-- Fatty acids, total trans-G: string (nullable = true)\n",
      " |-- Fiber, soluble-G: string (nullable = true)\n",
      " |-- Fiber, total dietary-G: string (nullable = true)\n",
      " |-- Folate, total-UG: string (nullable = true)\n",
      " |-- Folic acid-UG: string (nullable = true)\n",
      " |-- Iron, Fe-MG: string (nullable = true)\n",
      " |-- Magnesium, Mg-MG: string (nullable = true)\n",
      " |-- Manganese, Mn-MG: string (nullable = true)\n",
      " |-- Niacin-MG: string (nullable = true)\n",
      " |-- Pantothenic acid-MG: string (nullable = true)\n",
      " |-- Phosphorus, P-MG: string (nullable = true)\n",
      " |-- Potassium, K-MG: string (nullable = true)\n",
      " |-- Protein-G: string (nullable = true)\n",
      " |-- Riboflavin-MG: string (nullable = true)\n",
      " |-- Sodium, Na-MG: string (nullable = true)\n",
      " |-- Sugars, added-G: string (nullable = true)\n",
      " |-- Sugars, total including NLEA-G: string (nullable = true)\n",
      " |-- Thiamin-MG: string (nullable = true)\n",
      " |-- Total lipid (fat)-G: string (nullable = true)\n",
      " |-- Total sugar alcohols-G: string (nullable = true)\n",
      " |-- Vitamin A, IU-IU: string (nullable = true)\n",
      " |-- Vitamin B-12-UG: string (nullable = true)\n",
      " |-- Vitamin B-6-MG: string (nullable = true)\n",
      " |-- Vitamin C, total ascorbic acid-MG: string (nullable = true)\n",
      " |-- Vitamin D (D2 + D3), International Units-IU: string (nullable = true)\n",
      " |-- Vitamin D (D2 + D3)-UG: string (nullable = true)\n",
      " |-- Zinc, Zn-MG: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac2352-1ff0-4545-a5ad-11486e18c841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456bc55-2e45-4ed5-94e5-833e17640404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = spark.createDataFrame([(\"sykorole\", 1), (\"test\", 2)], \"value STRING, fdc_id INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e0ffc7-0f9b-4440-9064-3ceded279c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc2bbf-e504-41f3-8bca-b753c672c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b228599-20e7-44a0-a6f1-f16311f950e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938123f-205a-4d1f-8641-291790d09d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14c0f7b9-babc-4788-b2d6-787240c74561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"spark-kafka-topic\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fe03311-6423-4cdb-b103-754c629c24d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87addbb6-83a8-4565-83d6-64771888b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 16:56:42 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------------+-----------------+---------+------+-----------------------+-------------+\n",
      "|key     |value                             |topic            |partition|offset|timestamp              |timestampType|\n",
      "+--------+----------------------------------+-----------------+---------+------+-----------------------+-------------+\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |0     |2024-06-20 16:37:24.872|0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |1     |2024-06-20 16:37:27.701|0            |\n",
      "|rabbitmq|{\"type\": \"push\", \"version\": \"2.5\"}|spark-kafka-topic|0        |2     |2024-06-20 16:40:16.39 |0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |3     |2024-06-20 16:43:58.51 |0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |4     |2024-06-20 16:51:43.41 |0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |5     |2024-06-20 16:52:00.52 |0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |6     |2024-06-20 16:52:01.191|0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |7     |2024-06-20 16:52:01.832|0            |\n",
      "|kafka   |{\"type\": \"pull\", \"version\": \"3.4\"}|spark-kafka-topic|0        |8     |2024-06-20 16:52:02.357|0            |\n",
      "|NULL    |test test test                    |spark-kafka-topic|0        |9     |2024-06-20 16:56:03.376|0            |\n",
      "+--------+----------------------------------+-----------------+---------+------+-----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"string\")) \\\n",
    "    .withColumn(\"key\", F.col(\"key\").cast(\"string\")) \\\n",
    "    .show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d24fc-aeee-49f6-b252-91ee136669e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e1d87-a2df-49bd-97b0-a67b3bf71fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "495636a4-c1b7-4878-b2d7-e837c1957e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"string\")) \\\n",
    "    .withColumn(\"key\", F.col(\"key\").cast(\"string\")) \\\n",
    "    .withColumn(\"type\", F.get_json_object(\"value\", \"$.type\")) \\\n",
    "    .withColumn(\"version\", F.get_json_object(\"value\", \"$.version\")) \\\n",
    "    .withColumnRenamed(\"key\", \"tool\") \\\n",
    "    .select(\"tool\", \"type\", \"version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29540b55-7a45-4f97-8146-e2c79b114084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 16:44:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------+\n",
      "|tool    |type|version|\n",
      "+--------+----+-------+\n",
      "|kafka   |pull|3.4    |\n",
      "|kafka   |pull|3.4    |\n",
      "|rabbitmq|push|2.5    |\n",
      "|kafka   |pull|3.4    |\n",
      "+--------+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b53ae2e2-f7d9-4184-a706-0db6dea065c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tool: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- version: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3a3a2b13-3d30-4e06-a145-77addb114022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd2f77d-2351-4be2-8dbb-c41480a641ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6aae1-50fc-4113-a5ab-212c7a50cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka:{\"type\": \"pull\", \"version\": \"3.4\"}\n",
    "rabbitmq:{\"type\": \"push\", \"version\": \"2.5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4099b-701b-46ed-aa6c-efba4308489a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522e8a3-128f-4aff-8a81-97d163c0cb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "576bdb47-0af1-48db-a473-b23c010c40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamDF = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"spark-kafka-topic\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e25a148c-f23f-4cac-ad63-eeab6c0c28f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "561e1a17-5ea2-4bc2-8aa4-f23758cdd2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e83db-a787-4110-957f-cfcb4dcd4be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9767bc-0511-4e5a-8adb-9edbb31e6a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba56725-406f-4264-877c-9535a2b75824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb55e60-a4d5-419a-b630-e059c6a6f4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dad8b535-7f9f-4a49-a004-d28bbd5df3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamDF = streamDF \\\n",
    "    .withColumn(\"value\", F.col(\"value\").cast(\"string\")) \\\n",
    "    .withColumn(\"key\", F.col(\"key\").cast(\"string\")) \\\n",
    "    .withColumn(\"type\", F.get_json_object(\"value\", \"$.type\")) \\\n",
    "    .withColumn(\"version\", F.get_json_object(\"value\", \"$.version\")) \\\n",
    "    .withColumnRenamed(\"key\", \"tool\") \\\n",
    "    .select(\"tool\", \"type\", \"version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee5e03-0e24-4094-b36c-7936eb72a3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "449967d0-78af-40fb-8b39-9c79360db495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 16:48:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-93abc3a0-389c-4c7d-8aff-9f6765fbcc5d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/20 16:48:29 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = streamDF.writeStream \\\n",
    "    .queryName(\"result\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcefa709-700a-4b0f-a012-ce432e1b5ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+\n",
      "| tool|type|version|\n",
      "+-----+----+-------+\n",
      "|kafka|pull|    3.4|\n",
      "|kafka|pull|    3.4|\n",
      "|kafka|pull|    3.4|\n",
      "|kafka|pull|    3.4|\n",
      "|kafka|pull|    3.4|\n",
      "+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df76c7-1991-464f-8626-0a346f4fb053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d02a25-fdf6-4b74-93c7-82c7713c3246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "938d0fe4-e77b-48fd-9e25-b976b4cece8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6235155b-cc21-4fd2-9e6d-f1f85e899098",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamDF = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"spark-kafka-topic\") \\\n",
    "  .option(\"startingOffsets\", \"latest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03719c81-8025-45bf-bfda-8ace06445cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7d477-34fd-4fb0-887e-8c3a961ffea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a947b40c-e3a0-4a40-9c05-d4b1a13afc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamDF_grouped = streamDF \\\n",
    "    .filter(F.col(\"key\").isNull()) \\\n",
    "    .select(\"value\") \\\n",
    "    .withColumn(\"timestamp\", F.current_timestamp()) \\\n",
    "    .withColumn(\"value\", F.split(\"value\", \" \", -1)) \\\n",
    "    .withColumn(\"value\", F.explode(F.col(\"value\"))) \\\n",
    "    .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(F.window(\"timestamp\", \"30 seconds\"), \"value\") \\\n",
    "    .agg(F.count(\"*\").alias(\"cnt\")) \\\n",
    "    .orderBy(\"cnt\") \\\n",
    "    .join(df_dict, on=\"value\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459f397-641c-4789-b538-2ff04707c297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2fb18f-8600-4366-accf-6fee236d174b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c993c68-6069-4c5e-9d50-71312f089553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 17:42:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bb3cd7b3-1ce4-4d9e-aee3-b03aafbfc416. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/20 17:42:40 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "q2 = streamDF_grouped.writeStream \\\n",
    "    .queryName(\"result\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b8b4ec9-e885-4772-8c66-f78629dbc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------+---+----+\n",
      "|value   |window                                    |cnt|id  |\n",
      "+--------+------------------------------------------+---+----+\n",
      "|sykorole|{2024-06-20 17:43:00, 2024-06-20 17:43:30}|2  |1   |\n",
      "|good    |{2024-06-20 17:42:30, 2024-06-20 17:43:00}|1  |NULL|\n",
      "|good    |{2024-06-20 17:43:00, 2024-06-20 17:43:30}|1  |NULL|\n",
      "|text    |{2024-06-20 17:42:30, 2024-06-20 17:43:00}|1  |NULL|\n",
      "|test    |{2024-06-20 17:43:00, 2024-06-20 17:43:30}|7  |1   |\n",
      "|test    |{2024-06-20 17:42:30, 2024-06-20 17:43:00}|2  |1   |\n",
      "+--------+------------------------------------------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da2c61-84c7-4538-85d3-837c6a60f36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128206a-f30f-4fe5-9a6f-db143a67fa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1069b-6c80-4460-95c1-fa8909fc8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ce7ddef-d093-4131-ae67-f619ae30b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d129c94c-c0f9-41c4-8560-3875fa16a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = spark.createDataFrame([(\"sykorole\", 1), (\"test\", 1)], \"value STRING, id INT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76482c89-e2c0-4416-9394-23c4f09b0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|   value| id|\n",
      "+--------+---+\n",
      "|sykorole|  1|\n",
      "|    test|  1|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf01b8-763d-447f-8bcf-3db267cbdf90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
