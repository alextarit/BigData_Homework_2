{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b06d19-8478-4df4-bb2d-99dfa4f07aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1909f1-a5fa-4393-8213-a0c8bca6525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"Wgnl0MKcUDKSa39rJb4u\"\n",
    "SECRET_KEY = \"rT6wUVf6XTpN1DDY032dcY9oeHvupsSgpMkpg1I7\"\n",
    "MINIO_URL = \"http://minio:9000\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"HW2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", True) \\\n",
    "    .config(\"spark.executor.memory\", \"1000M\") \\\n",
    "    .config(\"spark.driver.memory\", \"600M\") \\\n",
    "    .config('spark.jars.packages', \n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-pom:1.12.365,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e91ca2-3dc5-4605-9393-b303508a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9ece4-83c7-486a-84b6-192fca40d533",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "## Входные данные \n",
    "- Файл с данными по оттоку телеком оператора в США (churn.csv)\n",
    "- Справочник с названиями штатов (state.json)\n",
    "- Справочник с численностью населения территорий (определяется полем area code) внутри штатов (state.json)\n",
    "- Террия с численностью населения меньше 10_000 считается **мелкой**\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Посчитать количество отточных и неотточных абонентов (поле churn), исключив **мелкие** территории\n",
    "2. Отчет должен быть выполнен в разрезе **каждого штата** с его полным наименованием\n",
    "3. Описать возникающие узкие места при выполнении данной операции\n",
    "4. Применить один из способов оптимизации для ускорения выполнения запроса (при допущении, что справочник численности населения **сильно меньше** основных данных)\n",
    "5. Если существует еще какой-то способ, применить также и его отдельно от п.4 (при допущении, что справочник численности населения **сопоставим по размеру** с основными данными)\n",
    "6. Кратко описать реализованные способы и в чем их практическая польза\n",
    "\n",
    "- P.S. Одним из выбранных способов должен быть Bucket specific join\n",
    "- P.P.S. При обосновании предлагаем прикладывать запуска команды df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58da6b-e2ff-4cd6-9995-c4d35e5c2745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/churn.csv\")\n",
    "state_dict = spark.read.json(\"s3a://input/data/state.json\")\n",
    "pop_dict = spark.read.json(\"s3a://input/data/population.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18763c6",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568286cb-1279-4f2d-86e3-ff1fc6fd822d",
   "metadata": {},
   "source": [
    "Проведем сначала небольшой разведочный анализ (EDA) для понимания того, что хранится в каждом нашем датафрейме spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d1246-26ca-40e1-905f-85c51a582dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f8082-79f9-403e-a741-0a580bf700f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fcfd3-ed79-4053-befe-54f19a0f0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899da56e-0919-49e2-bbe2-787d612cca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dict.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1711af-56d8-44e6-9b3f-d1e6bc08cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dict.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#зафиксируем трешхолд для последующей оптимизации\n",
    "POP_THRESHOLD = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2795d9d-1f9c-4581-9800-956238ce973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#уберем мелкие территории, так называемый метод Predicate pushdown перенесли шаг фильтрации ближе к источнику\n",
    "churn_and_pop = churn_df.join(pop_dict, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad1f57-b204-41b6-b4df-0c405b2e6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = churn_and_pop.join(state_dict, churn_and_pop.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce5657-696d-489c-96cd-7bb1db563d68",
   "metadata": {},
   "source": [
    "Следует перечислить следующие узкие места:\n",
    "* т.к. наши файлы pop_dict и state_dict имеют размерность точно меньше <10МБ , то при соединении мы можем использовать BroadcastJoinThreshold было бы тут эффективнее\n",
    "* из прочего пункта поскольку у нас sort merge join, то по плану видно, что имеется четыре операции перемешивания exchange до операции merge(логично было бы что два поскольку два join, поэтому вопросики к catalyst в данном случае либо ко мне) \n",
    "* сильный перекос в timeline в spark ui stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc29e32-581e-480b-b298-923d0fe80ce5",
   "metadata": {},
   "source": [
    "### Оптимизация 1 - broadcast has join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52085180-7fd5-4054-afdc-b4bd56c5ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dict_broad = F.broadcast(pop_dict)\n",
    "state_dict_broad = F.broadcast(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5a785-7687-43cc-9416-341e8ec36907",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_and_pop_broad = churn_df.join(pop_dict_broad, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7afd3b-75f5-4351-9c8d-e3816edb2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_broad = churn_and_pop_broad.join(state_dict_broad, churn_and_pop_broad.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea922ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_broad.explain()\n",
    "result_broad.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadb995-3def-430f-8574-acdba40312b7",
   "metadata": {},
   "source": [
    "Broadcast hash join дает преимущество в виде скорости за счет того, что данные содержит локально на каждом executor и нет необходимости производить сетевую операцию - shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac80fd0",
   "metadata": {},
   "source": [
    "### Оптимизация 2 - bucket specific join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c18f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#бакитируем один из датафреймов чтобы репартирование при join происходило без shuffle и sort\n",
    "churn_df.repartition(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(100, col=\"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/bucket/churn_bucket\") \\\n",
    "    .saveAsTable(\"churn_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fcd9c-e6fe-4e03-b3c4-7f6438c16b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_dict \\\n",
    "    .withColumn(\"area code\", F.col(\"area code\").cast(\"string\")) \\\n",
    "    .repartition(1) \\\n",
    "    .write \\\n",
    "    .bucketBy(100, \"area code\") \\\n",
    "    .option(\"path\", \"s3a://input/data/bucket/pop_bucket\") \\\n",
    "    .saveAsTable(\"pop_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d446130",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_bucket = spark.table(\"churn_bucket\")\n",
    "pop_bucket = spark.table(\"pop_bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_and_pop_bucket = churn_bucket.join(pop_bucket, on=\"area code\", how=\"inner\") \\\n",
    "    .filter(F.col(\"population\") > POP_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77aadfd-a5c2-4636-9de1-a0057e90442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bucket = churn_and_pop_bucket.join(state_dict, churn_and_pop_bucket.state==state_dict.state_id, how=\"left\") \\\n",
    "    .groupBy(\"state_name\") \\\n",
    "    .pivot(\"churn\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .orderBy(\"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7aef35-f09e-4edd-9cd4-f37af560ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bucket.explain()\n",
    "result_bucket.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91681ca-4e23-4f36-aee5-e6afdf432395",
   "metadata": {},
   "source": [
    "Видим по плану запроса, что у нас отсутвует shuffle по area code для таблиц pop_bucket и churn_bucket, тем самым мы получили прирост производительсности по времени."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f06364",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "## Входные данные \n",
    "\n",
    "*skew_transactions.csv* - информация о длительности просомтра контента пользователям\n",
    "колонки:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2. element_uid — уникальный идентификатор контента\n",
    "3. watched_time — время просмотра в секундах\n",
    "\n",
    "*catalogue.json* - каталог с описанием контента и метаинформации по нему\n",
    "колонки:\n",
    "1. type — тип элемента\n",
    "2. duration — длительность в минутах (средняя длительность эпизода в случае с сериалами и многосерийными фильмами), округлённая до десятков\n",
    "3. attributes — анонимизированные атрибуты данного элемента\n",
    "4. availability — доступные права на элемент(subscription, purchase, rent)\n",
    "5. feature_1 — анонимизированная вещественная переменная\n",
    "6. feature_2 — анонимизированная вещественная переменная\n",
    "7. feature_3 — анонимизированная порядковая переменная\n",
    "8. feature_4 — анонимизированная вещественная переменная\n",
    "9. feature_5 — анонимизированная вещественная переменная\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Выполните join основных данных со справочником используя DataFrame API (по колонке id для контента - `element_uid`)\n",
    "2. Описать проблему в датасетах с точки зрения обработки Spark\n",
    "3. Решить задачу любым способом\n",
    "4. Решить задачу с помощью salt-join подхода\n",
    "\n",
    "P.S. Как вы можете заметить при просмотре данных по пользователями, нужный нам ключ для операции будет перекошен (90% строк представлены на фильм, очень популярный среди смотревших) - это нужно доказать в рамках п.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740055d",
   "metadata": {},
   "source": [
    "### Решение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b7736-7ca2-4aed-9eec-56b0f0a49616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a6177-92eb-4fc7-b3d0-56c99884a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/skew_transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ab026-99a9-45db-8aca-8fc28de3a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_df = pd.read_json(\"datasets/catalogue.json\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b73fb-2822-4b78-ab90-4b4c8342a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27137082-dbd6-4d0f-b395-ae94975c522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#транспонируем датафрейм и также индекс обозначим как столбец element_uid\n",
    "\n",
    "catalogue_df = catalogue_df.transpose().reset_index()\n",
    "catalogue_df = catalogue_df.rename(columns={\"index\": \"element_uid\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ba115-5065-4ef2-b8b0-defafeb470c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c57789-2837-4a93-aad6-bae5c16ef6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#посмотрим на уникальные значения\n",
    "catalogue_df[\"element_uid\"].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d6131-1901-487f-907e-3416879120ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_df = spark.createDataFrame(catalogue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f375b0a-12be-4c57-a790-2a6d2944d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cfbfb-c6f7-4686-bbf8-7066cf741ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#также посмотрим на униклаьные значения element_uid, чтобы увидеть что распределение столбца element_uid перекошено\n",
    "skew_df.groupBy(\"element_uid\").count().orderBy(F.col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3993c-0cfe-4ded-80de-66af5c0098fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = skew_df.join(catalogue_df.select(\"element_uid\", \"type\", \"availability\", \"duration\"), on=\"element_uid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.explain()\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666db5d-22b9-49a1-81d3-63bda36a0a47",
   "metadata": {},
   "source": [
    "Как видно по плану у нас выполняется дефолтный sort merge join, мы имеем shuffle, а с учетом перекоса ключа есть следующие проблемы:\n",
    "* **Излишняя нагрузка на некоторые узлы**: В sort-merge join данные должны быть отсортированы и объединены по ключам. Если ключи распределены неравномерно, узлы, отвечающие за частые ключи, получат больше данных для обработки. Это приводит к дисбалансу нагрузки(судя по spark ui timeline), когда некоторые узлы будут перегружены, становясь узкими местами (bottlenecks), а другие будут простаивать.\n",
    "* **Использование памяти в режиме memory spill**: Не в данном кейсе, но в целом, перегруженные узлы могут исчерпать доступную память из-за большого объема данных, что может привести к сбоям или необходимости использования дискового пространства для выполнения операций, что замедляет процесс.\n",
    "* **Большое количество операций shuffle**: Операция происходит между узлами, что прямо сказывается на производительсности и скорости выполенения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c868f",
   "metadata": {},
   "source": [
    "### Решение с оптимизацией - broadcast hash join\n",
    "Опять же поскольку одна из таблиц, а конкретнее catalogue_df у нас 2.7Mb < 10Mb (смотрел в хранилище minio, не отрицаю возможно есть более релевантный способ), то мы можем использовать broadcast hash join чтобы избежать лишнего shuffle и следовательно лишнего сетевого взаимодействия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_broad = F.broadcast(catalogue_df)\n",
    "skew_repartition = skew_df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d63037-019b-4eba-9df2-b75fa6917dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_broad = skew_repartition.join(catalogue_broad.select(\"element_uid\", \"type\", \"availability\", \"duration\"), on=\"element_uid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_broad.explain()\n",
    "result_broad.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd280877-33b7-4708-9e85-2c792cfef037",
   "metadata": {},
   "source": [
    "Мы получили прирост производительности:\n",
    "* было - 2с\n",
    "* стало - 0.5с\n",
    "Конечно кидает warning, но в данной задаче он прям не критичен. Мы избегаем этап shuffle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e7ce2-d098-47e5-a532-7527ad72451d",
   "metadata": {},
   "source": [
    "### Решение с оптимизацией - salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ee170-ac6a-4690-9640-a5685d6f42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_skew_helper(left, right, key, number_of_partitions, how=\"inner\"):\n",
    "    #generate random salt for left dataframe\n",
    "    salt_value = F.lit(F.rand() * number_of_partitions % number_of_partitions).cast(\"int\")\n",
    "    left = left.withColumn(\"salt\", salt_value)\n",
    "\n",
    "    #Creating a column with an array of all possible salt values for right dataframe\n",
    "    salt_col = F.explode(F.array([F.lit(i) for i in range(number_of_partitions)])).alias(\"salt\")\n",
    "    right = right.select(\"*\",  salt_col)\n",
    "\n",
    "    return left.join(right, [key, \"salt\"], how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce214c3-63cb-45f5-bfde-72da9eb9b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_salt = data_skew_helper(skew_df, catalogue_df.select(\"element_uid\", \"type\", \"availability\", \"duration\"), \"element_uid\", 5, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba801a0-c72d-4515-b864-a0bf959503f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_salt.explain()\n",
    "result_salt.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95d0af-794a-4344-b16e-93f6e7a6751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтобы убедиться что распределение по партициям у нас лучше, чем при простом merge sort join без salt\n",
    "result_salt_check_partition = result_salt.groupBy(\"element_uid\", \"salt\").count().orderBy(F.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245441f-b946-468c-954a-b5c4591054f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_salt_check_partition.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadfd83-5ccc-4ee3-a092-7e1c8d67d1db",
   "metadata": {},
   "source": [
    "В данном случае мы получили время выполнения 0.6с, что быстрее по сравнению с раннее двумя использованными способами. Это происходит за счет того, что происходит смягчение перекоса данных, что позволяет равномерно распределить данные по нескольким ключам, тем самым помогая сбалансировать нагрузку между узлами. Но как видим по плану работ shuffle при этом у нас сохраняется как при простом sort merge join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898809",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "\n",
    "## Входные данные \n",
    "\n",
    "*cut_transactions.csv*  — информация о длительности просомтра контента пользователям\n",
    "\n",
    "Описание фичей в cut_transactions.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2.  element_uid — уникальный идентификатор контента\n",
    "3.  watched_time — время просмотра в секундах\n",
    "\n",
    "*cut_ratings.csv*  — информация об оценках, поставленных пользователями\n",
    "\n",
    "Описание фичей в cut_ratings.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя \n",
    "2. element_uid — уникальный идентификатор контента \n",
    "3. rating — поставленный пользователем рейтинг\n",
    "\n",
    "*ids.csv*  — выборка пользователей\n",
    "Описание фичей в ids.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя \n",
    "\n",
    "\n",
    "## Что нужно сделать\n",
    "Для каждого пользователя из выборки посчитать:\n",
    "1. Максимальное и минимальное время просмотра фильмов с оценками 8, 9 и 10 \n",
    "2. Название фичи должно быть в формате feat_агрегирующая_функция_watched_time_rating_оценка. \n",
    "3. Если у пользователь не ставил оценки 8, 9 и 10 то значение фичей должно быть null\n",
    "4. Описать принятые при разработки кода решения и возможные оптимизации\n",
    "\n",
    "P.S. На каждом этапе обработки должно быть должны агрегироваться минимально возможные объемы данных (сокращаем затраты на shuflle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb99364",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/cut_transactions.csv\")\n",
    "ratings_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/cut_ratings.csv\")\n",
    "ratings_df = F.broadcast(ratings_df)\n",
    "sample = spark.read.option(\"header\", True).csv(\"s3a://input/data/ids.csv\")\n",
    "sample = F.broadcast(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff739948-4e41-41e7-b61c-7e241dbda95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42ecf5-93cc-44bb-8d16-ebe87f9b9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122835f3-eee3-429c-8269-4aec38fe8035",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ce30c-78b2-45a9-80aa-8b03da439d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab664140-099b-4748-ae6b-26a505f0b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b52674-ed4f-4397-ab29-1e8bd826ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0963ed4",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dac489",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table = transactions_df.join(ratings_df , on=[\"user_uid\", \"element_uid\"], how=\"inner\").join(sample, on=\"user_uid\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75bdcde-1fa6-4988-b1f6-00a3093afda4",
   "metadata": {},
   "source": [
    "**Непосредственно решение нашей задачи**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3c7a1-9cac-451a-97ce-93dfe1560ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_table = full_table.filter(F.col(\"rating\").isin([8, 9, 10]))\n",
    "full_table_filter = filtered_table.groupBy(\"user_uid\", \"rating\") \\\n",
    "                                  .agg(F.min(\"watched_time\").alias(\"min_watched_time\"), \n",
    "                                       F.max(\"watched_time\").alias(\"max_watched_time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0436d25e-d3e9-4493-ad9c-c4500d99a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fulfilling the requirement of column format \n",
    "ratings = [8, 9, 10]\n",
    "result_df = sample\n",
    "\n",
    "for rating in ratings:\n",
    "    rating_df = full_table_filter.filter(F.col(\"rating\") == rating) \\\n",
    "                                 .select(\n",
    "                                     F.col(\"user_uid\"),\n",
    "                                     F.col(\"min_watched_time\").alias(f\"feat_min_watched_time_rating_{rating}\"),\n",
    "                                     F.col(\"max_watched_time\").alias(f\"feat_max_watched_time_rating_{rating}\")\n",
    "                                 )\n",
    "    result_df = result_df.join(rating_df, on=\"user_uid\", how=\"left\")\n",
    "\n",
    "# Sort table\n",
    "result_table = result_df.orderBy(\"user_uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2a31c-7ced-4c52-93c9-6d0b129e5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table.explain()\n",
    "result_table.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a99401-1104-4e6e-ad77-544f611ecd8b",
   "metadata": {},
   "source": [
    "Опять же использовали подход с использованием broadcast hash join, поскольку наша табличка sample и cut_ratings небольшая, то это позволяет избежать shuffle операций и значительно ускорить join операции."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
